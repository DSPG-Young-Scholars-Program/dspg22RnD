---
title: "Data"
---
<!-- ```{python} -->
<!-- import numpy as np -->
<!-- from sklearn.feature_extraction.text import CountVectorizer -->
<!-- from sklearn.feature_extraction.text import TfidfTransformer -->
<!-- from sklearn.neighbors import KNeighborsClassifier -->
<!-- from sklearn.pipeline import Pipeline -->

<!-- import pandas as pd -->
<!-- pd.set_option('display.max_columns', 50) -->

<!-- import numpy as np -->
<!-- import pickle -->
<!-- import matplotlib.pyplot as plt -->
<!-- import time -->
<!-- import re -->


<!-- from sklearn.decomposition import NMF -->
<!-- from sklearn.feature_extraction.text import TfidfVectorizer -->
<!-- from sklearn.linear_model import LinearRegression as lm -->
<!-- from scipy import stats -->

<!-- import matplotlib.collections as mcol -->
<!-- from matplotlib.legend_handler import HandlerLineCollection, HandlerTuple -->
<!-- from matplotlib.lines import Line2D -->

<!-- from sklearn.preprocessing import StandardScaler -->
<!-- from sklearn.preprocessing import MinMaxScaler -->
<!-- from sklearn.preprocessing import RobustScaler -->

<!-- df = pd.read_pickle("/home/seh6fy/git/publicrd/data/prd/Paper/FR_meta_and_final_tokens_23DEC21.pkl") -->
<!-- df = df[~df.DEPARTMENT.isnull()] -->
<!-- df_nsf = df.loc[df['DEPARTMENT'] == 'HHS'] -->
<!-- df_nsf = df_nsf.iloc[0:10000,] -->
<!-- df_nasa = df.loc[df['DEPARTMENT'] == 'NSF'] -->
<!-- df_nasa = df_nasa.iloc[0:10000,] -->
<!-- frames = [df_nsf, df_nasa] -->
<!-- df_split = pd.concat(frames) -->
<!-- df_split= df_split.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill='') -->
<!-- from sklearn.feature_extraction.text import TfidfVectorizer -->
<!-- v = TfidfVectorizer() -->
<!-- training = v.fit_transform(df_split['ABSTRACT']) -->
<!-- training_data = df_split.sample(frac=0.95, random_state=25) -->
<!-- testing_data = df_split.drop(training_data.index) -->
<!-- train_id = training_data.index -->
<!-- test_id = testing_data.index -->
<!-- x_train= training[train_id] -->
<!-- x_test= training[test_id] -->
<!-- df_split['index'] = df_split.index -->
<!-- y_test = df_split.loc[df_split["index"].isin(test_id)] -->
<!-- y_test = y_test['DEPARTMENT'] -->
<!-- df_split['index'] = df_split.index -->
<!-- y_train = df_split.loc[df_split["index"].isin(train_id)] -->
<!-- y_train = y_train['DEPARTMENT'] -->
<!-- import math -->
<!-- neighbors = int(math.sqrt(len(df_split.index))) -->
<!-- from sklearn.neighbors import KNeighborsClassifier -->
<!-- classifier = KNeighborsClassifier(n_neighbors= 3) -->
<!-- classifier.fit(x_train, y_train) -->
<!-- y_pred = classifier.predict(x_test) -->
<!-- import scipy.sparse -->
<!-- predictions = df_split.loc[test_id,] -->
<!-- predictions['PREDICTIONS'] = y_pred -->
<!-- predictions = predictions[["index","DEPARTMENT", "PREDICTIONS","ABSTRACT"]] -->
<!-- ``` -->

# Text Vectorization Data






## Text Classifier Data
### KNN on TF-IDF Vectorization
```{r echo=FALSE}
library(rmarkdown)
#paged_table(read.csv("/home/seh6fy/git/rnd/src/dataframe.csv"))
```
Above can be seen the predictions made with a KNN classifier run on a TF-IDF data set. It can be seen that the classifier predicted that the abstracts were both big data and not big data.  
```{r echo=FALSE}
library(rmarkdown)
# library(dplyr)
#df <-read.csv("/home/seh6fy/git/rnd/src/dataframe.csv")

#table(df['PREDICTIONS'])

#table(df['DEPARTMENT'])
```
The first table above represents the counts of the predictions as to whether an abstract in the training set was big data or not. The second table represents the number of actual data in the training set that were big data or not. 
